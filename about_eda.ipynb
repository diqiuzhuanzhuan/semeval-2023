{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task2.data_man.meta_data import ConllItem\n",
    "from typing import AnyStr, Dict\n",
    "from task2.data_man.meta_data import _assign_ner_tags, extract_spans, get_wiki_knowledge, join_tokens, get_wiki_title_knowledge, get_wiki_title_google_type, get_wiki_entities\n",
    "from task2.data_man.meta_data import read_conll_item_from_file, write_json_gzip\n",
    "from task2.configuration.config import logging\n",
    "from task2.configuration import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_coverage(conll_file: AnyStr, entity_vocab: Dict, type: AnyStr='ORG'):\n",
    "    conll_items = read_conll_item_from_file(conll_file)\n",
    "    hit_count = 0\n",
    "    total_count = 0 + 1e-6\n",
    "    error_conunt = 0\n",
    "    for item in conll_items:\n",
    "        gold_spans = extract_spans(item.labels)\n",
    "        gold_entities = []\n",
    "        gold_labels = []\n",
    "        for k in gold_spans:\n",
    "            if gold_spans[k] == 'O':\n",
    "                continue\n",
    "            gold_entities.append(join_tokens(item.tokens[k[0]:k[1]+1])[0])\n",
    "            gold_labels.append(item.labels[k[0]][2:])\n",
    "        #logging.info('{}'.format(join_tokens(tokens)[0]))    \n",
    "        #logging.info('gold entities: {}'.format(gold_entities))\n",
    "        #logging.info('gold labels: {}'.format(gold_labels))\n",
    "    \n",
    "        for i, entity in enumerate(gold_entities):\n",
    "            if entity in entity_vocab:\n",
    "                if gold_labels[i] == type:\n",
    "                    hit_count += 1\n",
    "                    total_count += 1\n",
    "                    if type not in entity_vocab[entity]:\n",
    "                        logging.info(entity)\n",
    "                        logging.info(entity_vocab[entity])\n",
    "                        #logging.info(item.id)\n",
    "                        logging.info('{}'.format(join_tokens(item.tokens)[0]))    \n",
    "                        error_conunt += 1\n",
    "            else:\n",
    "                if gold_labels[i] == type:\n",
    "                    #logging.info('{}'.format(join_tokens(item.tokens)[0]))    \n",
    "                    #logging.info('{}: {} is not in'.format(entity, gold_labels[i]))\n",
    "                    total_count += 1\n",
    "    logging.info(\"error count:{}, error_rate: {}%\".format(error_conunt, round(error_conunt/total_count, 4) *100))\n",
    "    return '{}%'.format(round(hit_count / total_count, 4) * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_entities_vocab = get_wiki_entities(config.wiki_entity_data['person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in person_entities_vocab:\n",
    "    person_entities_vocab[k] = list(set(person_entities_vocab[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task2.data_man.meta_data import write_json_gzip\n",
    "write_json_gzip(config.wiki_entity_data['person'], person_entities_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for type in ['Cleric', 'Scientist', 'Artist', 'OtherPER']:\n",
    "    if type != 'Cleric':\n",
    "        continue\n",
    "    coverage = analyze_coverage(config.train_file['Chinese'], person_entities_vocab, type=type)\n",
    "    logging.info('{} coverage is {}'.format(type, coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_entities_vocab['喬佛里·波吉亞']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "group_entities = get_wiki_entities(config.wiki_entity_data['group'])\n",
    "GROUP_TYPE = {\n",
    "    'Q4438121': 'sport organization',  \n",
    "    'Q936518': 'aerospace manufacturer',\n",
    "    'Q215380': 'musical group',\n",
    "    'Q32178211': 'musical group',\n",
    "    'Q891723': 'public corporation',\n",
    "    'Q7257717': 'public corporation',\n",
    "    'Q5621421': 'private corparation',\n",
    "    'Q786820': 'automobile manufacturer',\n",
    "    'Q18388277': 'technology company',\n",
    "    'Q43229':'orginazation',\n",
    "    'Q4830453':'orginazation',\n",
    "}\n",
    "type_by_id = collections.defaultdict(list)\n",
    "for dir in os.walk('group_en_20230106'):\n",
    "\n",
    "    for l in dir:\n",
    "        if not isinstance(l, list):\n",
    "            continue\n",
    "        for file in l:\n",
    "            if not file.endswith('csv'):\n",
    "                continue\n",
    "            csv_file = './group_en_20230106/{}'.format(file)\n",
    "            qid = csv_file.split('_')[3]\n",
    "            data = pd.read_csv(csv_file, sep='\\t', header=None)\n",
    "            type = GROUP_TYPE[qid]\n",
    "            for row in data.iterrows():\n",
    "                entity = row[1].values[1]\n",
    "                if re.match(r'Q[0-9]+', entity):\n",
    "                    continue\n",
    "                if entity in group_entities and type == 'orginazation':\n",
    "                    continue\n",
    "                if entity in group_entities and type != 'orginazation':\n",
    "                    if type not in group_entities[entity]:\n",
    "                        group_entities[entity].append(type)\n",
    "                if entity not in group_entities:\n",
    "                    group_entities[entity] = [type]\n",
    "\n",
    "print(len(group_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_entities = get_wiki_entities(config.wiki_entity_data['group'])\n",
    "from task2.data_man.meta_data import write_json_gzip\n",
    "#write_json_gzip(config.wiki_entity_data['group'], json_dict=group_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = analyze_coverage(config.validate_file['English'], group_entities, type='PublicCorp')\n",
    "logging.info('coverage is {}'.format(coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('med_zh.json', 'r', encoding='utf-8') as f: \n",
    "    data = json.load(f)\n",
    "    write_json_gzip(config.wiki_entity_data['medicine'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('loc_zh.json', 'r', encoding='utf-8') as f: \n",
    "    data = json.load(f)\n",
    "    write_json_gzip(config.wiki_entity_data['location'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pro_zh.json', 'r', encoding='utf-8') as f: \n",
    "    data = json.load(f)\n",
    "    write_json_gzip(config.wiki_entity_data['product'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cre_zh.json', 'r', encoding='utf-8') as f: \n",
    "    data = json.load(f)\n",
    "    write_json_gzip(config.wiki_entity_data['creative work'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_wiki_entities(config.wiki_entity_data['creative work'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_wiki_entities(config.wiki_entity_data['creative work'])\n",
    "coverage = analyze_coverage(config.validate_file['Chinese'], data, type='WrittenWork')\n",
    "logging.info('coverage is {}'.format(coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"<visual work>\")['input_ids'])\n",
    "print(tokenizer('<medicine>')['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(['visualwork'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dict_230111/group_12_language_dict.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "group_data = get_wiki_entities(config.wiki_entity_data['group'])\n",
    "for k in data:\n",
    "    group_data[k] = data[k]\n",
    "write_json_gzip(config.wiki_entity_data['group'], group_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_230111/location_zh_language_dict.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "group_data = get_wiki_entities(config.wiki_entity_data['location'])\n",
    "for k in data:\n",
    "    group_data[k] = data[k]\n",
    "write_json_gzip(config.wiki_entity_data['location'], group_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_230111/product_zh_language_dict.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "group_data = get_wiki_entities(config.wiki_entity_data['product'])\n",
    "for k in data:\n",
    "    group_data[k] = data[k]\n",
    "write_json_gzip(config.wiki_entity_data['product'], group_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python semeval2023",
   "language": "python",
   "name": "semeval2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0aabd0f4680e6d910ea2edc0dd61e8c46bc5e7a1cbed81d898c12970ea0b6e4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
